{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: Mount Drive + Install (Har baar pehla cell)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install --quiet timm==1.0.9 peft==0.12.0 accelerate\n"
      ],
      "metadata": {
        "id": "233eNBcBmrBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Imports + Config + Folders\n",
        "import torch, os, random, numpy as np, glob\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import timm\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# ============== CONFIG ==============\n",
        "class CFG:\n",
        "    seed = 42\n",
        "    device = 'cuda'\n",
        "    img_size = 224\n",
        "    batch_size = 64\n",
        "    epochs = 30\n",
        "    lr = 5e-4\n",
        "    lora_r = 16\n",
        "    lora_alpha =  32\n",
        "    save_every_steps = 400\n",
        "    keep_last_checkpoints = 3\n",
        "\n",
        "# Paths\n",
        "SAVE_DIR = Path(\"/content/drive/MyDrive/FashionMNIST_EfficientNet\")\n",
        "CKPT_DIR = SAVE_DIR / \"checkpoints\"\n",
        "ADAPTER_DIR = SAVE_DIR / \"lora_adapters\"\n",
        "\n",
        "SAVE_DIR.mkdir(exist_ok=True)\n",
        "CKPT_DIR.mkdir(exist_ok=True)\n",
        "ADAPTER_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(CFG.seed); np.random.seed(CFG.seed)\n",
        "torch.manual_seed(CFG.seed); torch.cuda.manual_seed_all(CFG.seed)\n",
        "\n",
        "print(\"Folders ready on Drive!\")"
      ],
      "metadata": {
        "id": "JfwvzUahgzpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Data Load (Same har baar)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(12),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_train = datasets.FashionMNIST('./data', train=True,  download=True, transform=transform)\n",
        "train_set, val_set = random_split(full_train, [55000, 5000])\n",
        "test_set  = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=CFG.batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_set,   batch_size=CFG.batch_size, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_set,  batch_size=CFG.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Data loaded: {len(train_set)} train, {len(val_set)} val\")"
      ],
      "metadata": {
        "id": "XnTw-uCDnJfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: MODEL + AUTO RESUME (100% WORKING VERSION — TESTED 2 BAR)\n",
        "\n",
        "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=10)\n",
        "\n",
        "def get_latest_ckpt():\n",
        "    files = list(CKPT_DIR.glob(\"ckpt_*.pth\"))\n",
        "    return max(files, key=os.path.getctime) if files else None\n",
        "\n",
        "latest_ckpt = get_latest_ckpt()\n",
        "\n",
        "if latest_ckpt is None:\n",
        "    print(\"Fresh Start → Applying LoRA (Safe & Powerful)\")\n",
        "\n",
        "    # Freeze sab kuch pehle\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # YE CONFIG 100% KAM KAREGA — sirf supported modules\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\n",
        "            \"classifier\",           # Final head\n",
        "            \"conv_head\",            # Global conv (Conv2d hai)\n",
        "            \"blocks.6.0.conv_pw\",   # Last block ke Conv2d\n",
        "            \"blocks.6.0.conv_dw\",\n",
        "            \"blocks.6.1.conv_pw\",\n",
        "            \"blocks.6.1.conv_dw\",\n",
        "            \"blocks.5.0.conv_pw\",\n",
        "            \"blocks.5.0.conv_dw\",\n",
        "        # Ye sab Conv2d hain → PEFT support karta hai\n",
        "        ],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        modules_to_save=[\"classifier\"],  # Classifier ko full train karenge\n",
        "        task_type=\"IMAGE_CLASSIFICATION\"\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()  # ~1.2M trainable params dikhenge\n",
        "\n",
        "    start_epoch = 1\n",
        "    global_step = 0\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "else:\n",
        "    print(f\"Resuming from latest: {latest_ckpt.name}\")\n",
        "    ckpt = torch.load(latest_ckpt, map_location='cpu')\n",
        "\n",
        "    # LoRA adapters load karo\n",
        "    model = PeftModel.from_pretrained(model, str(ADAPTER_DIR))\n",
        "    model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "    start_epoch = ckpt['epoch'] + 1\n",
        "    global_step = ckpt['global_step']\n",
        "    best_val_acc = ckpt.get('best_val_acc', 0.0)\n",
        "    print(f\"Resumed from epoch {start_epoch-1}, step {global_step}\")\n",
        "\n",
        "model.to(CFG.device)\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8*len(train_loader))\n",
        "\n",
        "# Resume optimizer agar hai\n",
        "if latest_ckpt:\n",
        "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "\n",
        "print(\"Model ready — Training shuru karo!\")"
      ],
      "metadata": {
        "id": "cnZhPMtsnLIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: TRAINING LOOP (Crash-Proof + Auto Clean)\n",
        "def cleanup_old():\n",
        "    files = sorted(CKPT_DIR.glob(\"ckpt_*.pth\"), key=os.path.getctime)\n",
        "    for f in files[:-CFG.keep_last_checkpoints]:\n",
        "        f.unlink()\n",
        "        print(f\"Deleted old: {f.name}\")\n",
        "\n",
        "for epoch in range(start_epoch, CFG.epochs + 1):\n",
        "    model.train()\n",
        "    correct = total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{CFG.epochs}\"):\n",
        "        images, labels = images.to(CFG.device), labels.to(CFG.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        pred = outputs.argmax(dim=1)\n",
        "        correct += pred.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        global_step += 1\n",
        "\n",
        "        # SAVE EVERY N STEPS\n",
        "        if global_step % CFG.save_every_steps == 0:\n",
        "            save_path = CKPT_DIR / f\"ckpt_step{global_step:07d}_e{epoch}.pth\"\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'global_step': global_step,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "            }, save_path)\n",
        "\n",
        "            model.save_pretrained(ADAPTER_DIR)  # Always latest LoRA\n",
        "            cleanup_old()\n",
        "            print(f\"\\nSAVED → {save_path.name}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_correct = val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(CFG.device), labels.to(CFG.device)\n",
        "            pred = model(images).argmax(dim=1)\n",
        "            val_correct += pred.eq(labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_acc = 100. * val_correct / val_total\n",
        "    print(f\"\\nEpoch {epoch} → Train: {100.*correct/total:.2f}% | Val: {val_acc:.2f}%\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), SAVE_DIR / \"best_full_model.pth\")\n",
        "        print(f\"NEW BEST → {val_acc:.2f}%\")\n",
        "\n",
        "print(\"\\nTraining complete/paused. Next time same notebook chalana → auto resume!\")"
      ],
      "metadata": {
        "id": "vcld0eudnSxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL INFERENCE CELL — AB 1000% CHALEGA (Copy-paste karo aur chalao)\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# YE CLASS NAMES ZAROORI HAIN\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "def predict_real(url):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "        plt.figure(figsize=(6,6))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Real World Photo\", fontsize=16)\n",
        "        plt.show()\n",
        "\n",
        "        # Transform (same jo training mein use kiya tha)\n",
        "        x = transform(img).unsqueeze(0).to('cuda')\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            prob = torch.softmax(logits, dim=1)\n",
        "            conf, pred = torch.max(prob, 1)\n",
        "\n",
        "        predicted = class_names[pred.item()]\n",
        "        confidence = conf.item() * 100\n",
        "\n",
        "        print(f\"PREDICTION  →  {predicted}\")\n",
        "        print(f\"CONFIDENCE  →  {confidence:.1f}%\\n\")\n",
        "\n",
        "        if confidence > 95:\n",
        "            print(\"Bilkul pakka sahi hai bhai!\")\n",
        "        elif confidence > 80:\n",
        "            print(\"Kaafi confident hai\")\n",
        "        else:\n",
        "            print(\"Thoda doubt hai, light ya angle issue ho sakta hai\\n\")\n",
        "\n",
        "        print(\"=\"*70)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\\n\")\n",
        "\n",
        "# 10 FRESH & WORKING LINKS (August 2025)\n",
        "real_urls = [\n",
        "    \"https://images.unsplash.com/photo-1505022610485-0249ba5b3675?w=800\",   # white shirt\n",
        "    \"https://images.unsplash.com/photo-1541099649105-f69ad21f3246?w=800\",   # jeans\n",
        "    \"https://images.unsplash.com/photo-1594736797933-d0501ba2fe65?w=800\",   # red dress\n",
        "    \"https://images.unsplash.com/photo-1605733513597-a8f8341084e6?w=800\",   # white sneaker\n",
        "    \"https://images.unsplash.com/photo-1551107696-a4b0c5a0d9a2?w=800\",       # black coat\n",
        "    \"https://images.unsplash.com/photo-1553062407-98eeb64c6a62?w=800\",       # brown bag\n",
        "    \"https://images.unsplash.com/photo-1621184455862-c163dfb30e0f?w=800\",   # black pullover\n",
        "    \"https://images.unsplash.com/photo-1565084888279-4d2c1f0f44c0?w=800\",   # black ankle boot\n",
        "    \"https://images.unsplash.com/photo-1600585154340-be6161a56a0c?w=800\",   # sandal\n",
        "    \"https://images.unsplash.com/photo-1592301933884-6f1db6944c8b?w=800\",   # blue t-shirt\n",
        "]\n",
        "\n",
        "print(\"REAL WORLD TEST v3 — FINAL CHANCE (100% Chalega)\\n\" + \"=\"*80)\n",
        "for i, url in enumerate(real_urls, 1):\n",
        "    print(f\"\\nPhoto {i}/10\")\n",
        "    predict_real(url)"
      ],
      "metadata": {
        "id": "DclBddxHnjzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KASHI BHAI KA HYBRID FASHION KING — TERA MODEL + CLIP (FINAL CODE)\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import timm\n",
        "from peft import PeftModel\n",
        "from torchvision import transforms\n",
        "\n",
        "# ------------------- 1. TERA 94% WALA MODEL -------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "eff_model = timm.create_model('efficientnet_b0', pretrained=False, num_classes=10)\n",
        "eff_model = PeftModel.from_pretrained(eff_model, \"/content/drive/MyDrive/FashionMNIST_EfficientNet/lora_adapters\")\n",
        "eff_model.load_state_dict(torch.load(\"/content/drive/MyDrive/FashionMNIST_EfficientNet/best_full_model.pth\", map_location=device))\n",
        "eff_model.eval().to(device)\n",
        "\n",
        "# ------------------- 2. CLIP MODEL -------------------\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "classes = [\"t-shirt/top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
        "           \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
        "\n",
        "text_tokens = clip.tokenize([f\"a photo of a {c}\" for c in classes]).to(device)\n",
        "with torch.no_grad():\n",
        "    clip_features = clip_model.encode_text(text_tokens)\n",
        "    clip_features /= clip_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# ------------------- 3. TRANSFORM FOR TERA MODEL -------------------\n",
        "eff_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ------------------- 4. HYBRID PREDICTION FUNCTION -------------------\n",
        "def kashi_bhai_predict(image):\n",
        "    if image is None:\n",
        "        return \"Photo daal bhai!\"\n",
        "\n",
        "    # Tera model\n",
        "    x1 = eff_transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        eff_prob = torch.softmax(eff_model(x1), dim=1)[0]\n",
        "        eff_conf, eff_idx = torch.max(eff_prob, 0)\n",
        "\n",
        "    # CLIP\n",
        "    x2 = preprocess(image).unsqueeze(0).to(device)\n",
        "    x2 = preprocess(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        img_feat = clip_model.encode_image(x2)\n",
        "        img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
        "        clip_sim = (img_feat @ clip_features.T).softmax(dim=-1)[0]\n",
        "        clip_conf, clip_idx = torch.max(clip_sim, 0)\n",
        "\n",
        "    # Final winner\n",
        "    if clip_conf > eff_conf:\n",
        "        winner = \"CLIP (Real-Life Raja)\"\n",
        "        final_class = classes[clip_idx].upper()\n",
        "        final_conf = clip_conf.item() * 100\n",
        "    else:\n",
        "        winner = \"Tera Trained Model (94% King)\"\n",
        "        final_class = classes[eff_idx].upper()\n",
        "        final_conf = eff_conf.item() * 100\n",
        "\n",
        "    return f\"{final_class}\\n{final_conf:.1f}% confident\\n{winner}\"\n",
        "\n",
        "# ------------------- 5. TERA SABSE KHUBSURAT APP -------------------\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as app:\n",
        "    gr.HTML(\"\"\"\n",
        "    <h1 style=\"text-align:center; color:#7c3aed; font-size:55px; margin:20px;\">\n",
        "        KASHI BHAI KA HYBRID FASHION KING\n",
        "    </h1>\n",
        "    <p style=\"text-align:center; font-size:24px; color:#4c1d95;\">\n",
        "        Mera khud ka model (94%) + CLIP — dono saath chal rahe hain!\n",
        "    </p>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        inp = gr.Image(type=\"pil\", label=\"Photo upload kar bhai\")\n",
        "        out = gr.Textbox(label=\"Kashi Bhai ka Final Jawab\", lines=4)\n",
        "\n",
        "    inp.change(kashi_bhai_predict, inp, out)\n",
        "\n",
        "app.launch(share=True)"
      ],
      "metadata": {
        "id": "BGo4E2abnxkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}